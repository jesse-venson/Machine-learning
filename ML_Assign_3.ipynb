{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3YjnhTchv+raDMRS3WMCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesse-venson/Machine-learning/blob/main/ML_Assign_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "71ALKII9jIdi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import KFold, train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION **1**"
      ],
      "metadata": {
        "id": "FR3xMLWVky5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('USA_Housing.csv')\n",
        "X = df.drop('Price', axis = 1).values\n",
        "y = df['Price'].values"
      ],
      "metadata": {
        "id": "PL9j0hfjk2go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kf = KFold(n_splits = 5, shuffle = True, random_state=42)\n",
        "\n",
        "betas, r2_scores, y_preds = [],[],[]\n"
      ],
      "metadata": {
        "id": "akQospfkk4hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_idx , test_idx in kf.split(X_scaled):\n",
        "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    # Least Square fit\n",
        "    # adding intercept (bias term)\n",
        "    X_train2 = np.hstack([np.ones((X_train.shape[0],1)),X_train])\n",
        "    X_test2 = np.hstack([np.ones((X_test.shape[0],1)),X_test])\n",
        "\n",
        "    beta = np.linalg.pinv(X_train2.T @ X_train2) @ X_train2.T @ y_train\n",
        "    y_pred = X_test2 @ beta\n",
        "    r2 = r2_score(y_test,y_pred)\n",
        "    betas.append(beta)\n",
        "    r2_scores.append(r2)\n",
        "    y_preds.append(y_pred)"
      ],
      "metadata": {
        "id": "u6QVSEh6lFdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_idx = np.argmax(r2_scores)\n",
        "best_beta = betas[best_idx]\n"
      ],
      "metadata": {
        "id": "1tIPtL7nlFlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_70, X_test_30,y_train_70, y_test_30 = train_test_split(X_scaled, y, test_size = 0.3, random_state =42)\n",
        "X_train2_70 = np.hstack([np.ones((X_train_70.shape[0],1)),X_train_70])\n",
        "X_test2_30 = np.hstack([np.ones((X_test_30.shape[0],1)),X_test_30])"
      ],
      "metadata": {
        "id": "W64r2XmrlIAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_70 = np.linalg.pinv(X_train2_70.T @ X_train2_70) @ X_train2_70.T @ y_train_70"
      ],
      "metadata": {
        "id": "RoAHr1L3lIIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_30 = X_test2_30 @ beta_70\n",
        "r2_30 = r2_score(y_test_30,y_pred_30)"
      ],
      "metadata": {
        "id": "195RCfVClINd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question **2**"
      ],
      "metadata": {
        "id": "lo17QKDZlOny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Price', axis = 1)\n",
        "y = df['Price']"
      ],
      "metadata": {
        "id": "chTyJUCQlQ8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "nF40Dl0BlS1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the validation set : we split only the training subset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "j8eAureylUVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, learning_rate, iterations):\n",
        "    m, n = X.shape\n",
        "    # Initialize weights (including bias)\n",
        "    weights = np.zeros(n)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        predictions = X.dot(weights)\n",
        "        errors = predictions - y\n",
        "        gradient = (1/m) * X.T.dot(errors)\n",
        "        weights -= learning_rate * gradient\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "NFkdnTFglWhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_intercept(X):\n",
        "    intercept = np.ones((X.shape[0], 1))\n",
        "    return np.hstack((intercept, X))\n",
        "\n",
        "X_train_i = add_intercept(X_train)\n",
        "X_val_i = add_intercept(X_val)\n",
        "X_test_i = add_intercept(X_test)"
      ],
      "metadata": {
        "id": "yiPXtJIPlZdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "learning_rates = [0.001, 0.01, 0.1, 1]\n",
        "iterations = 1000\n",
        "best_lr = None\n",
        "best_weights = None\n",
        "best_val_r2 = -np.inf\n",
        "\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    weights = gradient_descent(X_train_i, y_train, lr, iterations)\n",
        "\n",
        "    # Predict on validation and test sets\n",
        "    val_pred = X_val_i.dot(weights)\n",
        "    test_pred = X_test_i.dot(weights)\n",
        "\n",
        "    print(np.isnan(X_train_i).sum(), np.isnan(X_val_i).sum(), np.isnan(X_test_i).sum())\n",
        "    print(np.isnan(y_train).sum(), np.isnan(y_val).sum(), np.isnan(y_test).sum())\n",
        "\n",
        "    val_r2 = r2_score(y_val, val_pred)\n",
        "    test_r2 = r2_score(y_test, test_pred)\n",
        "\n",
        "    results.append({\n",
        "        'learning_rate': lr,\n",
        "        'weights': weights,\n",
        "        'val_r2': val_r2,\n",
        "        'test_r2': test_r2\n",
        "    })\n",
        "\n",
        "    # Track best model by validation R2\n",
        "    if val_r2 > best_val_r2:\n",
        "        best_val_r2 = val_r2\n",
        "        best_weights = weights\n",
        "        best_lr = lr\n",
        "\n",
        "print(f\"Best learning rate: {best_lr}\")\n",
        "print(f\"Best validation R2: {best_val_r2}\")\n",
        "print(f\"Test R2 at best learning rate: {[r['test_r2'] for r in results if r['learning_rate'] == best_lr][0]}\")\n",
        "print(f\"Best regression coefficients: {best_weights}\")"
      ],
      "metadata": {
        "id": "GEJ43bztlbEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question **3**"
      ],
      "metadata": {
        "id": "iUvAnR_DlmLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\"\n",
        "columns = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\"num_doors\", \"body_style\",\n",
        "           \"drive_wheels\", \"engine_location\", \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
        "           \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\", \"bore\", \"stroke\",\n",
        "           \"compression_ratio\", \"horsepower\", \"peak_rpm\", \"city_mpg\", \"highway_mpg\", \"price\"]\n",
        "\n",
        "df = pd.read_csv(url, names=columns, na_values='?')"
      ],
      "metadata": {
        "id": "7a1EsJ_Ylnn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "ws7jioEClrhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols.remove('price')\n",
        "\n",
        "# for columns with numerical values and imputing them with mean\n",
        "for col in numeric_cols:\n",
        "    df[col].fillna(df[col].astype(float).mean(), inplace=True)\n",
        "\n",
        "# # for columns with categorical values and imputing them with most occurent value (mode)\n",
        "for col in categorical_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "gAbzowSulsN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_num = {\n",
        "    'two': 2,\n",
        "    'three': 3,\n",
        "    'four': 4,\n",
        "    'five': 5,\n",
        "    'six': 6,\n",
        "    'eight': 8,\n",
        "    'twelve': 12\n",
        "}\n",
        "\n",
        "df['num_doors'] = df['num_doors'].map(words_to_num)\n",
        "df['num_cylinders'] = df['num_cylinders'].map(words_to_num)"
      ],
      "metadata": {
        "id": "Et1A1kavluEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns=['body_style', 'drive_wheels'], drop_first=True)"
      ],
      "metadata": {
        "id": "KFp1XrHTlw1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_cols = ['make', 'aspiration', 'engine_location', 'fuel_type']\n",
        "le = LabelEncoder()\n",
        "\n",
        "for col in label_cols:\n",
        "    df[col] = le.fit_transform(df[col])"
      ],
      "metadata": {
        "id": "Y5zkaS3HlzMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dtypes[df.dtypes == 'object'])"
      ],
      "metadata": {
        "id": "9XJBjgvVl0lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['fuel_system'] = df['fuel_system'].apply(lambda x: 1 if 'pfi' in x else 0)\n",
        "df['engine_type'] = df['engine_type'].apply(lambda x: 1 if 'ohc' in x else 0)"
      ],
      "metadata": {
        "id": "yhRZMK1yl2KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price'].astype(float)  # convert price to float\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "xVL-EIM8l4Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isna().sum())  # Should print 0"
      ],
      "metadata": {
        "id": "NsPUG3Nal5tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "print(f\"R2 score on test set: {r2_score(y_test, y_pred):.4f}\")\n",
        "print(f\"RMSE on test set: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")"
      ],
      "metadata": {
        "id": "H1pLsu2El8ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Retain enough components to explain 95% variance (or choose number manually)\n",
        "pca = PCA(0.95)  # 95% variance explained\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original number of features: {X_scaled.shape[1]}\")\n",
        "print(f\"Reduced number of features: {X_pca.shape[1]}\")\n",
        "\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
        "\n",
        "lr_model_pca = LinearRegression()\n",
        "lr_model_pca.fit(X_train_pca, y_train_pca)\n",
        "\n",
        "y_pred_pca = lr_model_pca.predict(X_test_pca)\n",
        "\n",
        "print(f\"R2 score on PCA test set: {r2_score(y_test_pca, y_pred_pca):.4f}\")\n",
        "print(f\"RMSE on PCA test set: {np.sqrt(mean_squared_error(y_test_pca, y_pred_pca)):.4f}\")"
      ],
      "metadata": {
        "id": "cTYuCrOTl_sc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}