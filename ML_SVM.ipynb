{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPApeWlJNeMB4xcknaoWmUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesse-venson/Machine-learning/blob/main/ML_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1 - Part A: Load Dataset and Train-Test Split"
      ],
      "metadata": {
        "id": "9DZmWR_GPmC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlHId5wlPiNU",
        "outputId": "89db7b8c-3114-480f-d448-4f82f6104a33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully\n",
            "Training samples: 120\n",
            "Test samples: 30\n",
            "Features: 4\n",
            "Classes: ['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split (80:20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Dataset loaded successfully\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "print(f\"Features: {X_train.shape[1]}\")\n",
        "print(f\"Classes: {iris.target_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q1 - Part B, C, D: Train SVM with Different Kernels and Evaluate"
      ],
      "metadata": {
        "id": "q9Fcr6woPtkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "results = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training SVM with {kernel.upper()} kernel\")\n",
        "    print('='*70)\n",
        "\n",
        "    # Train SVM\n",
        "    if kernel == 'poly':\n",
        "        svm = SVC(kernel=kernel, degree=3, random_state=42)\n",
        "    else:\n",
        "        svm = SVC(kernel=kernel, random_state=42)\n",
        "\n",
        "    svm.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Store results\n",
        "    results[kernel] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "# Find best kernel\n",
        "best_kernel = max(results, key=lambda k: results[k]['accuracy'])\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"BEST KERNEL: {best_kernel.upper()}\")\n",
        "print(f\"Accuracy: {results[best_kernel]['accuracy']:.4f}\")\n",
        "print('='*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGyJ7ef4PwVR",
        "outputId": "2026c6f8-aacd-4500-85ca-6b8ad864b8a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Training SVM with LINEAR kernel\n",
            "======================================================================\n",
            "\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-Score: 1.0000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  0 10]]\n",
            "\n",
            "======================================================================\n",
            "Training SVM with POLY kernel\n",
            "======================================================================\n",
            "\n",
            "Accuracy: 0.9667\n",
            "Precision: 0.9697\n",
            "Recall: 0.9667\n",
            "F1-Score: 0.9666\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  1]\n",
            " [ 0  0 10]]\n",
            "\n",
            "======================================================================\n",
            "Training SVM with RBF kernel\n",
            "======================================================================\n",
            "\n",
            "Accuracy: 0.9667\n",
            "Precision: 0.9697\n",
            "Recall: 0.9667\n",
            "F1-Score: 0.9666\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  1]\n",
            " [ 0  0 10]]\n",
            "\n",
            "======================================================================\n",
            "BEST KERNEL: LINEAR\n",
            "Accuracy: 1.0000\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1 - Part E: Identify Best Kernel and Explain"
      ],
      "metadata": {
        "id": "ZbjcksFAPz7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Kernel':<15} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
        "print(\"-\"*70)\n",
        "for kernel in kernels:\n",
        "    print(f\"{kernel.upper():<15} {results[kernel]['accuracy']:<12.4f} {results[kernel]['precision']:<12.4f} {results[kernel]['recall']:<12.4f} {results[kernel]['f1_score']:<12.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"EXPLANATION:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best kernel: {best_kernel.upper()}\")\n",
        "print(\"\\nWhy it performs best:\")\n",
        "if best_kernel == 'linear':\n",
        "    print(\"- Linear kernel works well because Iris data is linearly separable\")\n",
        "    print(\"- Simple decision boundaries are sufficient\")\n",
        "    print(\"- Less prone to overfitting with small datasets\")\n",
        "elif best_kernel == 'rbf':\n",
        "    print(\"- RBF kernel can capture non-linear relationships\")\n",
        "    print(\"- Flexible decision boundaries adapt to data structure\")\n",
        "    print(\"- Good generalization on complex patterns\")\n",
        "elif best_kernel == 'poly':\n",
        "    print(\"- Polynomial kernel captures polynomial relationships\")\n",
        "    print(\"- Degree 3 provides moderate complexity\")\n",
        "    print(\"- Can model curved decision boundaries\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsUD34KhP1xs",
        "outputId": "3fda30ec-d8fc-4ce0-e0c4-193ce0edd429"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "SUMMARY COMPARISON\n",
            "======================================================================\n",
            "Kernel          Accuracy     Precision    Recall       F1-Score    \n",
            "----------------------------------------------------------------------\n",
            "LINEAR          1.0000       1.0000       1.0000       1.0000      \n",
            "POLY            0.9667       0.9697       0.9667       0.9666      \n",
            "RBF             0.9667       0.9697       0.9667       0.9666      \n",
            "\n",
            "======================================================================\n",
            "EXPLANATION:\n",
            "======================================================================\n",
            "Best kernel: LINEAR\n",
            "\n",
            "Why it performs best:\n",
            "- Linear kernel works well because Iris data is linearly separable\n",
            "- Simple decision boundaries are sufficient\n",
            "- Less prone to overfitting with small datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 - Part A, B: Breast Cancer with and without Scaling"
      ],
      "metadata": {
        "id": "PURARGpVP4oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SVM WITHOUT FEATURE SCALING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train SVM WITHOUT scaling\n",
        "svm_no_scale = SVC(kernel='rbf', random_state=42)\n",
        "svm_no_scale.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "train_acc_no_scale = accuracy_score(y_train, svm_no_scale.predict(X_train))\n",
        "test_acc_no_scale = accuracy_score(y_test, svm_no_scale.predict(X_test))\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc_no_scale:.4f}\")\n",
        "print(f\"Testing Accuracy: {test_acc_no_scale:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SVM WITH FEATURE SCALING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM WITH scaling\n",
        "svm_with_scale = SVC(kernel='rbf', random_state=42)\n",
        "svm_with_scale.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate\n",
        "train_acc_with_scale = accuracy_score(y_train, svm_with_scale.predict(X_train_scaled))\n",
        "test_acc_with_scale = accuracy_score(y_test, svm_with_scale.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc_with_scale:.4f}\")\n",
        "print(f\"Testing Accuracy: {test_acc_with_scale:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Metric':<25} {'Without Scaling':<20} {'With Scaling':<20}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Training Accuracy':<25} {train_acc_no_scale:<20.4f} {train_acc_with_scale:<20.4f}\")\n",
        "print(f\"{'Testing Accuracy':<25} {test_acc_no_scale:<20.4f} {test_acc_with_scale:<20.4f}\")\n",
        "print(f\"{'Improvement':<25} {'-':<20} {test_acc_with_scale - test_acc_no_scale:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsuAJ9qNP7t3",
        "outputId": "408c0347-59c0-4196-dd7c-a3ea4d9c6135"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "SVM WITHOUT FEATURE SCALING\n",
            "======================================================================\n",
            "Training Accuracy: 0.9187\n",
            "Testing Accuracy: 0.9298\n",
            "\n",
            "======================================================================\n",
            "SVM WITH FEATURE SCALING\n",
            "======================================================================\n",
            "Training Accuracy: 0.9824\n",
            "Testing Accuracy: 0.9825\n",
            "\n",
            "======================================================================\n",
            "COMPARISON\n",
            "======================================================================\n",
            "Metric                    Without Scaling      With Scaling        \n",
            "----------------------------------------------------------------------\n",
            "Training Accuracy         0.9187               0.9824              \n",
            "Testing Accuracy          0.9298               0.9825              \n",
            "Improvement               -                    +0.0526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 - Part C: Discussion on Feature Scaling Effect"
      ],
      "metadata": {
        "id": "y4SVDkKsP-EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DISCUSSION: EFFECT OF FEATURE SCALING ON SVM PERFORMANCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. Why Feature Scaling Matters for SVM:\")\n",
        "print(\"   - SVM uses distance-based calculations (kernel functions)\")\n",
        "print(\"   - Features with larger ranges dominate the distance metric\")\n",
        "print(\"   - Without scaling, the hyperplane is biased toward high-magnitude features\")\n",
        "\n",
        "print(\"\\n2. Impact on RBF Kernel:\")\n",
        "print(\"   - RBF kernel: exp(-gamma * ||x - x'||^2)\")\n",
        "print(\"   - Unscaled features cause numerical instability\")\n",
        "print(\"   - Small gamma values may not capture patterns effectively\")\n",
        "\n",
        "print(\"\\n3. Observed Results:\")\n",
        "print(f\"   - Without scaling: {test_acc_no_scale:.4f} test accuracy\")\n",
        "print(f\"   - With scaling: {test_acc_with_scale:.4f} test accuracy\")\n",
        "print(f\"   - Improvement: {(test_acc_with_scale - test_acc_no_scale)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n4. Conclusion:\")\n",
        "print(\"   - Feature scaling is ESSENTIAL for SVM, especially with RBF/polynomial kernels\")\n",
        "print(\"   - StandardScaler ensures all features contribute equally\")\n",
        "print(\"   - Always scale features before training SVM models\")"
      ],
      "metadata": {
        "id": "GgYXUuREQBmV",
        "outputId": "dcf474e1-aefa-4d4b-b5a9-019c3a19fe92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DISCUSSION: EFFECT OF FEATURE SCALING ON SVM PERFORMANCE\n",
            "======================================================================\n",
            "\n",
            "1. Why Feature Scaling Matters for SVM:\n",
            "   - SVM uses distance-based calculations (kernel functions)\n",
            "   - Features with larger ranges dominate the distance metric\n",
            "   - Without scaling, the hyperplane is biased toward high-magnitude features\n",
            "\n",
            "2. Impact on RBF Kernel:\n",
            "   - RBF kernel: exp(-gamma * ||x - x'||^2)\n",
            "   - Unscaled features cause numerical instability\n",
            "   - Small gamma values may not capture patterns effectively\n",
            "\n",
            "3. Observed Results:\n",
            "   - Without scaling: 0.9298 test accuracy\n",
            "   - With scaling: 0.9825 test accuracy\n",
            "   - Improvement: 5.26%\n",
            "\n",
            "4. Conclusion:\n",
            "   - Feature scaling is ESSENTIAL for SVM, especially with RBF/polynomial kernels\n",
            "   - StandardScaler ensures all features contribute equally\n",
            "   - Always scale features before training SVM models\n"
          ]
        }
      ]
    }
  ]
}